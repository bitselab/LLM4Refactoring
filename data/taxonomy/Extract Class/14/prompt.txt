Extract class refactorings are frequently employed to decompose complex classes to improve readability.
##################################################
An extract class refactoring pattern is to extract a few attributes and/or methods as a new class to decompose this class. Here is an example of the extract class refactoring that follows this pattern.
The source code before refactoring is:
```
public class ScanTargetPattern {
    private File _directory;
    private List<String> _includes = Collections.emptyList();
    private List<String> _excludes = Collections.emptyList();

    /**
     * @return the _directory
     */
    public File getDirectory() {
        return _directory;
    }

    /**
     * @param directory the directory to set
     */
    public void setDirectory(File directory) {
        this._directory = directory;
    }

    public void setIncludes(List<String> includes) {
        _includes= includes;
    }

    public void setExcludes(List<String> excludes) {
        _excludes = excludes;
    }

    public List<String> getIncludes() {
        return _includes;
    }

    public List<String> getExcludes() {
        return _excludes;
    }

}
```
The source code after refactoring is:
```
public class ScanPattern {
    private List<String> _includes = Collections.emptyList();
    private List<String> _excludes = Collections.emptyList();

    public void setIncludes(List<String> includes) {
        _includes = includes;
    }

    public void setExcludes(List<String> excludes) {
        _excludes = excludes;
    }

    public List<String> getIncludes() {
        return _includes;
    }

    public List<String> getExcludes() {
        return _excludes;
    }
}

public class ScanTargetPattern {
    private File _directory;
    private ScanPattern _pattern;

    /**
     * @return the _directory
     */
    public File getDirectory() {
        return _directory;
    }

    /**
     * @param directory the directory to set
     */
    public void setDirectory(File directory) {
        this._directory = directory;
    }

    public void setIncludes(List<String> includes) {
        if (_pattern == null)
            _pattern = new ScanPattern();
        _pattern.setIncludes(includes);
    }

    public void setExcludes(List<String> excludes) {
        if (_pattern == null)
            _pattern = new ScanPattern();
        _pattern.setExcludes(excludes);
    }

    public List<String> getIncludes() {
        return (_pattern == null ? Collections.emptyList() : _pattern.getIncludes());
    }

    public List<String> getExcludes() {
        return (_pattern == null ? Collections.emptyList() : _pattern.getExcludes());
    }

}
```
In this example, the developer extracted the following attributes and methods:
```
private List<String> _includes = Collections.emptyList();
private List<String> _excludes = Collections.emptyList();

public void setIncludes(List<String> includes) {
    _includes = includes;
}

public void setExcludes(List<String> excludes) {
    _excludes = excludes;
}

public List<String> getIncludes() {
    return _includes;
}

public List<String> getExcludes() {
    return _excludes;
}
```
as new class "ScanPattern".
##################################################
As a developer, imagine your team leader requests you to review a piece of code to identify potential extract class refactoring opportunities that follows the refactoring pattern above. The original code snippet is as follows:
```
package org.apache.lucene.index;

import java.io.IOException;
import org.apache.lucene.util.ByteBlockPool;
import org.apache.lucene.util.BytesRef;
import org.apache.lucene.util.BytesRefHash;
import org.apache.lucene.util.BytesRefHash.BytesStartArray;
import org.apache.lucene.util.Counter;
import org.apache.lucene.util.IntBlockPool;

/**
 * This class stores streams of information per term without knowing the size of the stream ahead of
 * time. Each stream typically encodes one level of information like term frequency per document or
 * term proximity. Internally this class allocates a linked list of slices that can be read by a
 * {@link ByteSliceReader} for each term. Terms are first deduplicated in a {@link BytesRefHash}
 * once this is done internal data-structures point to the current offset of each stream that can be
 * written to.
 */
abstract class TermsHashPerField implements Comparable<TermsHashPerField> {
    private static final int HASH_INIT_SIZE = 4;

    private final TermsHashPerField nextPerField;
    private final IntBlockPool intPool;
    final ByteBlockPool bytePool;
    // for each term we store an integer per stream that points into the bytePool above
    // the address is updated once data is written to the stream to point to the next free offset
    // in the terms stream. The start address for the stream is stored in
    // postingsArray.byteStarts[termId]
    // This is initialized in the #addTerm method, either to a brand new per term stream if the term
    // is new or
    // to the addresses where the term stream was written to when we saw it the last time.
    private int[] termStreamAddressBuffer;
    private int streamAddressOffset;
    private final int streamCount;
    private final String fieldName;
    final IndexOptions indexOptions;
    /* This stores the actual term bytes for postings and offsets into the parent hash in the case that this
     * TermsHashPerField is hashing term vectors.*/
    private final BytesRefHash bytesHash;

    ParallelPostingsArray postingsArray;
    private int lastDocID; // only with assert

    /**
     * streamCount: how many streams this field stores per term. E.g. doc(+freq) is 1 stream,
     * prox+offset is a second.
     */
    TermsHashPerField(
            int streamCount,
            IntBlockPool intPool,
            ByteBlockPool bytePool,
            ByteBlockPool termBytePool,
            Counter bytesUsed,
            TermsHashPerField nextPerField,
            String fieldName,
            IndexOptions indexOptions) {
        this.intPool = intPool;
        this.bytePool = bytePool;
        this.streamCount = streamCount;
        this.fieldName = fieldName;
        this.nextPerField = nextPerField;
        assert indexOptions != IndexOptions.NONE;
        this.indexOptions = indexOptions;
        PostingsBytesStartArray byteStarts = new PostingsBytesStartArray(this, bytesUsed);
        bytesHash = new BytesRefHash(termBytePool, HASH_INIT_SIZE, byteStarts);
    }

    void reset() {
        bytesHash.clear(false);
        sortedTermIDs = null;
        if (nextPerField != null) {
            nextPerField.reset();
        }
    }

    final void initReader(ByteSliceReader reader, int termID, int stream) {
        assert stream < streamCount;
        int streamStartOffset = postingsArray.addressOffset[termID];
        final int[] streamAddressBuffer =
                intPool.buffers[streamStartOffset >> IntBlockPool.INT_BLOCK_SHIFT];
        final int offsetInAddressBuffer = streamStartOffset & IntBlockPool.INT_BLOCK_MASK;
        reader.init(
                bytePool,
                postingsArray.byteStarts[termID] + stream * FIRST_LEVEL_SIZE,
                streamAddressBuffer[offsetInAddressBuffer + stream]);
    }

    private int[] sortedTermIDs;

    /**
     * Collapse the hash table and sort in-place; also sets this.sortedTermIDs to the results This
     * method must not be called twice unless {@link #reset()} or {@link #reinitHash()} was called.
     */
    final void sortTerms() {
        assert sortedTermIDs == null;
        sortedTermIDs = bytesHash.sort();
    }

    /** Returns the sorted term IDs. {@link #sortTerms()} must be called before */
    final int[] getSortedTermIDs() {
        assert sortedTermIDs != null;
        return sortedTermIDs;
    }

    final void reinitHash() {
        sortedTermIDs = null;
        bytesHash.reinit();
    }

    private boolean doNextCall;

    // Secondary entry point (for 2nd & subsequent TermsHash),
    // because token text has already been "interned" into
    // textStart, so we hash by textStart.  term vectors use
    // this API.
    private void add(int textStart, final int docID) throws IOException {
        int termID = bytesHash.addByPoolOffset(textStart);
        if (termID >= 0) { // New posting
            // First time we are seeing this token since we last
            // flushed the hash.
            initStreamSlices(termID, docID);
        } else {
            positionStreamSlice(termID, docID);
        }
    }

    /**
     * Called when we first encounter a new term. We must allocate slies to store the postings (vInt
     * compressed doc/freq/prox), and also the int pointers to where (in our {@link ByteBlockPool}
     * storage) the postings for this term begin.
     */
    private void initStreamSlices(int termID, int docID) throws IOException {
        // Init stream slices
        if (streamCount + intPool.intUpto > IntBlockPool.INT_BLOCK_SIZE) {
            // not enough space remaining in this buffer -- jump to next buffer and lose this remaining
            // piece
            intPool.nextBuffer();
        }

        if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto
                < (2 * streamCount) * FIRST_LEVEL_SIZE) {
            // can we fit at least one byte per stream in the current buffer, if not allocate a new one
            bytePool.nextBuffer();
        }

        termStreamAddressBuffer = intPool.buffer;
        streamAddressOffset = intPool.intUpto;
        intPool.intUpto += streamCount; // advance the pool to reserve the N streams for this term

        postingsArray.addressOffset[termID] = streamAddressOffset + intPool.intOffset;

        for (int i = 0; i < streamCount; i++) {
            // initialize each stream with a slice we start with ByteBlockPool.FIRST_LEVEL_SIZE)
            // and grow as we need more space. see ByteBlockPool.LEVEL_SIZE_ARRAY
            final int upto = newSlice(FIRST_LEVEL_SIZE);
            termStreamAddressBuffer[streamAddressOffset + i] = upto + bytePool.byteOffset;
        }
        postingsArray.byteStarts[termID] = termStreamAddressBuffer[streamAddressOffset];
        newTerm(termID, docID);
    }

    private boolean assertDocId(int docId) {
        assert docId >= lastDocID : "docID must be >= " + lastDocID + " but was: " + docId;
        lastDocID = docId;
        return true;
    }

    /**
     * Called once per inverted token. This is the primary entry point (for first TermsHash); postings
     * use this API.
     */
    void add(BytesRef termBytes, final int docID) throws IOException {
        assert assertDocId(docID);
        // We are first in the chain so we must "intern" the
        // term text into textStart address
        // Get the text & hash of this term.
        int termID = bytesHash.add(termBytes);
        // System.out.println("add term=" + termBytesRef.utf8ToString() + " doc=" + docState.docID + "
        // termID=" + termID);
        if (termID >= 0) { // New posting
            // Init stream slices
            initStreamSlices(termID, docID);
        } else {
            termID = positionStreamSlice(termID, docID);
        }
        if (doNextCall) {
            nextPerField.add(postingsArray.textStarts[termID], docID);
        }
    }

    private int positionStreamSlice(int termID, final int docID) throws IOException {
        termID = (-termID) - 1;
        int intStart = postingsArray.addressOffset[termID];
        termStreamAddressBuffer = intPool.buffers[intStart >> IntBlockPool.INT_BLOCK_SHIFT];
        streamAddressOffset = intStart & IntBlockPool.INT_BLOCK_MASK;
        addTerm(termID, docID);
        return termID;
    }

    final void writeByte(int stream, byte b) {
        int streamAddress = streamAddressOffset + stream;
        int upto = termStreamAddressBuffer[streamAddress];
        byte[] bytes = bytePool.getBuffer(upto >> ByteBlockPool.BYTE_BLOCK_SHIFT);
        assert bytes != null;
        int offset = upto & ByteBlockPool.BYTE_BLOCK_MASK;
        if (bytes[offset] != 0) {
            // End of slice; allocate a new one
            offset = allocSlice(bytes, offset);
            bytes = bytePool.buffer;
            termStreamAddressBuffer[streamAddress] = offset + bytePool.byteOffset;
        }
        bytes[offset] = b;
        (termStreamAddressBuffer[streamAddress])++;
    }

    final void writeBytes(int stream, byte[] b, int offset, int len) {
        final int end = offset + len;
        int streamAddress = streamAddressOffset + stream;
        int upto = termStreamAddressBuffer[streamAddress];
        byte[] slice = bytePool.getBuffer(upto >> ByteBlockPool.BYTE_BLOCK_SHIFT);
        assert slice != null;
        int sliceOffset = upto & ByteBlockPool.BYTE_BLOCK_MASK;

        while (slice[sliceOffset] == 0 && offset < end) {
            slice[sliceOffset++] = b[offset++];
            (termStreamAddressBuffer[streamAddress])++;
        }

        while (offset < end) {
            int offsetAndLength = allocKnownSizeSlice(slice, sliceOffset);
            sliceOffset = offsetAndLength >> 8;
            int sliceLength = offsetAndLength & 0xff;
            slice = bytePool.buffer;
            int writeLength = Math.min(sliceLength - 1, end - offset);
            System.arraycopy(b, offset, slice, sliceOffset, writeLength);
            sliceOffset += writeLength;
            offset += writeLength;
            termStreamAddressBuffer[streamAddress] = sliceOffset + bytePool.byteOffset;
        }
    }

    /**
     * An array holding the level sizes for byte slices. The first slice is 5 bytes, the second is 14,
     * and so on.
     */
    public static final int[] LEVEL_SIZE_ARRAY = {5, 14, 20, 30, 40, 40, 80, 80, 120, 200};

    /**
     * An array holding indexes for the {@link #LEVEL_SIZE_ARRAY}, to quickly navigate to the next
     * slice level. These are encoded on 4 bits in the slice, so the values in this array should be
     * less than 16.
     *
     * <p>{@code NEXT_LEVEL_ARRAY[x] == x + 1}, except for the last element, where {@code
     * NEXT_LEVEL_ARRAY[x] == x}, pointing at the maximum slice size.
     */
    public static final int[] NEXT_LEVEL_ARRAY = {1, 2, 3, 4, 5, 6, 7, 8, 9, 9};

    /** The first level size for new slices. */
    public static final int FIRST_LEVEL_SIZE = LEVEL_SIZE_ARRAY[0];

    /**
     * Allocates a new slice with the given size and level 0.
     *
     * @return the position where the slice starts
     */
    public int newSlice(final int size) {
        if (size > ByteBlockPool.BYTE_BLOCK_SIZE) {
            throw new IllegalArgumentException(
                    "Slice size "
                            + size
                            + " should be less than the block size "
                            + ByteBlockPool.BYTE_BLOCK_SIZE);
        }

        if (pool.byteUpto > ByteBlockPool.BYTE_BLOCK_SIZE - size) {
            pool.nextBuffer();
        }
        final int upto = pool.byteUpto;
        pool.byteUpto += size;
        pool.buffer[pool.byteUpto - 1] = 16; // This codifies level 0.
        return upto;
    }

    /**
     * Creates a new byte slice in continuation of the provided slice and return its offset into the
     * pool.
     *
     * @param slice the current slice
     * @param upto the offset into the current slice, which is expected to point to the last byte of
     *     the slice
     * @return the new slice's offset in the pool
     */
    public int allocSlice(final byte[] slice, final int upto) {
        return allocKnownSizeSlice(slice, upto) >> 8;
    }

    /**
     * Create a new byte slice in continuation of the provided slice and return its length and offset
     * into the pool.
     *
     * @param slice the current slice
     * @param upto the offset into the current slice, which is expected to point to the last byte of
     *     the slice
     * @return the new slice's length on the lower 8 bits and the offset into the pool on the other 24
     *     bits
     */
    public int allocKnownSizeSlice(final byte[] slice, final int upto) {
        final int level = slice[upto] & 15; // The last 4 bits codify the level.
        final int newLevel = NEXT_LEVEL_ARRAY[level];
        final int newSize = LEVEL_SIZE_ARRAY[newLevel];

        // Maybe allocate another block
        if (pool.byteUpto > ByteBlockPool.BYTE_BLOCK_SIZE - newSize) {
            pool.nextBuffer();
        }

        final int newUpto = pool.byteUpto;
        final int offset = newUpto + pool.byteOffset;
        pool.byteUpto += newSize;

        // Copy forward the past 3 bytes (which we are about to overwrite with the forwarding address).
        // We actually copy 4 bytes at once since VarHandles make it cheap.
        int past3Bytes = ((int) BitUtil.VH_LE_INT.get(slice, upto - 3)) & 0xFFFFFF;
        // Ensure we're not changing the content of `buffer` by setting 4 bytes instead of 3. This
        // should never happen since the next `newSize` bytes must be equal to 0.
        assert pool.buffer[newUpto + 3] == 0;
        BitUtil.VH_LE_INT.set(pool.buffer, newUpto, past3Bytes);

        // Write forwarding address at end of last slice:
        BitUtil.VH_LE_INT.set(slice, upto - 3, offset);

        // Write new level:
        pool.buffer[pool.byteUpto - 1] = (byte) (16 | newLevel);

        return ((newUpto + 3) << 8) | (newSize - 3);
    }

    final void writeVInt(int stream, int i) {
        assert stream < streamCount;
        while ((i & ~0x7F) != 0) {
            writeByte(stream, (byte) ((i & 0x7f) | 0x80));
            i >>>= 7;
        }
        writeByte(stream, (byte) i);
    }

    final TermsHashPerField getNextPerField() {
        return nextPerField;
    }

    final String getFieldName() {
        return fieldName;
    }

    private static final class PostingsBytesStartArray extends BytesStartArray {

        private final TermsHashPerField perField;
        private final Counter bytesUsed;

        private PostingsBytesStartArray(TermsHashPerField perField, Counter bytesUsed) {
            this.perField = perField;
            this.bytesUsed = bytesUsed;
        }

        @Override
        public int[] init() {
            if (perField.postingsArray == null) {
                perField.postingsArray = perField.createPostingsArray(2);
                perField.newPostingsArray();
                bytesUsed.addAndGet(
                        perField.postingsArray.size * (long) perField.postingsArray.bytesPerPosting());
            }
            return perField.postingsArray.textStarts;
        }

        @Override
        public int[] grow() {
            ParallelPostingsArray postingsArray = perField.postingsArray;
            final int oldSize = perField.postingsArray.size;
            postingsArray = perField.postingsArray = postingsArray.grow();
            perField.newPostingsArray();
            bytesUsed.addAndGet(postingsArray.bytesPerPosting() * (long) (postingsArray.size - oldSize));
            return postingsArray.textStarts;
        }

        @Override
        public int[] clear() {
            if (perField.postingsArray != null) {
                bytesUsed.addAndGet(
                        -(perField.postingsArray.size * perField.postingsArray.bytesPerPosting()));
                perField.postingsArray = null;
                perField.newPostingsArray();
            }
            return null;
        }

        @Override
        public Counter bytesUsed() {
            return bytesUsed;
        }
    }

    @Override
    public final int compareTo(TermsHashPerField other) {
        return fieldName.compareTo(other.fieldName);
    }

    /** Finish adding all instances of this field to the current document. */
    void finish() throws IOException {
        if (nextPerField != null) {
            nextPerField.finish();
        }
    }

    final int getNumTerms() {
        return bytesHash.size();
    }

    /**
     * Start adding a new field instance; first is true if this is the first time this field name was
     * seen in the document.
     */
    boolean start(IndexableField field, boolean first) {
        if (nextPerField != null) {
            doNextCall = nextPerField.start(field, first);
        }
        return true;
    }

    /** Called when a term is seen for the first time. */
    abstract void newTerm(int termID, final int docID) throws IOException;

    /** Called when a previously seen term is seen again. */
    abstract void addTerm(int termID, final int docID) throws IOException;

    /** Called when the postings array is initialized or resized. */
    abstract void newPostingsArray();

    /** Creates a new postings array of the specified size. */
    abstract ParallelPostingsArray createPostingsArray(int size);
}
```
If there are any refactoring opportunities, please do it and generate the refactored code. Otherwise, simply state that no refactoring is necessary.